# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w0meFYDPDEfOkyoq7qkjydOZz4sYLTZy
"""

!ls

# unzip data file
!unzip Data.zip

import os
import glob
import pandas as pd
import numpy as np
os.chdir("Data")

extension = 'csv'
all_filenames = [i for i in glob.glob('*.{}'.format(extension))]

# combine all files in the list
combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])
# export to csv
combined_csv.to_csv( "combined_csv.csv", index=False, encoding='utf-8-sig')

# read combined csv file
data= pd.read_csv('combined_csv.csv')

# check number of columns and rows
data.shape

# standardize the process and calculate std deviation and mean
def encode_numeric_zscore(df, name, mean=None, sd=None):
    if mean is None:
        mean = df[name].mean()

    if sd is None:
        sd = df[name].std()

    df[name] = (df[name] - mean) / sd

data= pd.read_csv('combined_csv.csv')
data.drop(columns=['id'],inplace=True)
mapping = {"AE":1, "NE":2, "NoE":3}   # mapped the data on the basis of events
data.loc[:,"label"] = data.marker.map(mapping) 
data.drop(columns=["marker"],inplace=True)  # drop the column with name marker
num_colums = data.columns

encode_numeric_zscore(data, num_colums)  
data.dropna(inplace=True, axis=1)  
data_shuffled = data.reindex(np.random.permutation(data.index))
x_columns = data_shuffled.columns.drop(['label']) # dropped the last column here
x = data_shuffled[x_columns].values 
dummies = pd.get_dummies(data_shuffled['label']) 
outcomes = dummies.columns
num_classes = len(outcomes)
y = dummies.values

# split and train the model in 80:20 ratio
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)

# use OneVsRestClassifier for SVM and use fit function on traning and testing dataset
from sklearn.svm import SVC
SVM = OneVsRestClassifier(kernel='linear')
SVM.fit(X_train, y_train)

# Calculate best parameters from SVM midel using hyperparameter tuning
from sklearn.datasets import load_iris
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score

model_to_set = OneVsRestClassifier(SVC(kernel="poly"))

parameters = {
    "estimator__C": [1,2,4,8],
    "estimator__kernel": ["poly","rbf"],
    "estimator__degree":[1, 2, 3, 4],
}

model_tunning = GridSearchCV(model_to_set, param_grid=parameters)

model_tunning.fit(x, y)
pred= SVM.predict(x)

from sklearn.multiclass import OneVsRestClassifier
import sklearn as sk
from sklearn import svm
import pandas as pd
SVM = OneVsRestClassifier(svm.LinearSVC())
SVM.fit(x, y)
pred= SVM.predict(x)

# Calculate the accuracy
SVM.classes_, SVM.multilabel_
SVM.score(x, y)

# Create classification report and calculate precision, recall, and f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y, pred)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y, pred, target_names=['Class 1', 'Class 2', 'Class 3']))

# Create confusion matrix
import sklearn.metrics as metrics
print(metrics.confusion_matrix(y.flatten(), pred.flatten()))

# Plot confusion matrix 
#import matplotlib.pyplot as plt
#from sklearn.metrics import plot_confusion_matrix
#plot_confusion_matrix(y, pred, labels=['Class 1', 'Class 2', 'Class 3'])
#plot_confusion_matrix(x, pred, y_true=y, labels=['Class 1', 'Class 2', 'Class 3'])

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y.argmax(axis =1), pred.argmax(axis =1))
plot_confusion_matrix(conf_mat=mat)

# data.isnull().values.any()

import pandas as pd
data = pd.read_csv('combined_csv.csv')
data.drop(columns=['id'],inplace=True)  # drop id column
data.head()

# Plot heatmap using matplotlib library
import matplotlib.pyplot as plt
import seaborn as sns

sample=data.sample(frac=0.1, replace=False, random_state=1)
plt.figure(figsize=(20,10))
c= sample.corr()
sns.heatmap(c,cmap="BrBG",annot=True)
c

# print all the columns
data.columns

# print data type of all the columns
columns=data.dtypes
columns

import seaborn as sns
import matplotlib.pyplot as plt
# data = data.drop_duplicates()

data.shape

mapping = {"AE":1, "NE":2, "NoE":3} # data mapping
data.loc[:,"label"] = data.marker.map(mapping) 
data.drop(columns=["marker"],inplace=True)

data.columns

num_colums = [x for x in data.columns if x not in ["label"]]

def encode_numeric_zscore(df, name, mean=None, sd=None):
    if mean is None:
        mean = df[name].mean()

    if sd is None:
        sd = df[name].std()

    df[name] = (df[name] - mean) / sd
num_colums = data.columns
encode_numeric_zscore(data, num_colums)  
data.dropna(inplace=True, axis=1)  
import numpy as np
data_shuffled = data.reindex(np.random.permutation(data.index))
x_columns = data_shuffled.columns.drop(['label']) #we have dropped the last column here
x = data_shuffled[x_columns].values 
dummies = pd.get_dummies(data_shuffled['label']) 
outcomes = dummies.columns
num_classes = len(outcomes)
y = dummies.values

num_colums = data.columns

encode_numeric_zscore(data, num_colums)
#encode_text_dummy(data, 'marker')
#encode_text_dummy(data,'label')

data.head() # print top 5 rows of dataset

data.dropna(inplace=True, axis=1) # drop any row with NA value

data.head()

import numpy as np
data_shuffled = data.reindex(np.random.permutation(data.index))
x_columns = data_shuffled.columns.drop(['label']) # dropped the last column here
x = data_shuffled[x_columns].values 
dummies = pd.get_dummies(data_shuffled['label']) 
outcomes = dummies.columns
num_classes = len(outcomes)
y = dummies.values

y.shape

# Commented out IPython magic to ensure Python compatibility.
# # KNN model
# %%time
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import roc_auc_score
# 
# # List Hyperparameters that we want to tune.
# leaf_size = list(range(1,10))
# n_neighbors = list(range(1,5))
# p=[1]
# 
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=51)
# 
# # Convert to dictionary
# hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)
# # Create new KNN object
# model = KNeighborsClassifier()
# # Use GridSearch
# clf = GridSearchCV(model, hyperparameters, cv=5)
# # Fit the model
# best_model = clf.fit(x_train,y_train)
# 
# # Print The value of best Hyperparameters
# print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])
# print('Best p:', best_model.best_estimator_.get_params()['p'])
# print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])
# 
# #model.fit(x_train,y_train)
# 
# #pred= model.predict(x_test)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.model_selection import train_test_split
# 
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=51)
# model = KNeighborsClassifier(n_neighbors=5)
# model.fit(x_train,y_train)
# pred= model.predict(x_test)

# fit and predict the model 
model.fit(x_train,y_train)
pred= model.predict(x_test)

import sklearn.metrics as metrics
print(metrics.confusion_matrix(y_test.flatten(), pred.flatten()))

from sklearn.metrics import roc_auc_score

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, pred)))  # calculate accuracy

# print('Micro Precision: {:.2f}'.format(precision_score(y_test, pred, average='micro')))
# print('Micro Recall: {:.2f}'.format(recall_score(y_test, pred, average='micro')))
# print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, pred, average='micro')))

# print('Macro Precision: {:.2f}'.format(precision_score(y_test, pred, average='macro')))
# print('Macro Recall: {:.2f}'.format(recall_score(y_test, pred, average='macro')))
# print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, pred, average='macro')))

# print('Weighted Precision: {:.2f}'.format(precision_score(y_test, pred, average='weighted')))
# print('Weighted Recall: {:.2f}'.format(recall_score(y_test, pred, average='weighted')))
# print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, pred, average='weighted')))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, pred, target_names=['Class 1', 'Class 2', 'Class 3']))  # create classification report
roc_auc_score(y_test, pred)

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test.argmax(axis =1), pred.argmax(axis =1))  # create confusion matrix
plot_confusion_matrix(conf_mat=mat) # plot confusion matrix

# Commented out IPython magic to ensure Python compatibility.
# # Random forest model
# %%time
# from sklearn.ensemble import RandomForestClassifier
# from scipy.stats import expon as sp_expon
# from scipy.stats import randint as sp_randint
# 
# #n_estimators = sp_expon(scale=100)
# #max_depth = sp_randint(1, 40)
# n_estimators = [10, 50, 100, 200]
# max_depth = [3, 10, 20, 40]
# 
# #rfc= RandomForestClassifier(n_estimators=50,criterion='gini',max_depth=5)
# rfc= RandomForestClassifier(n_estimators=100,criterion='gini',max_depth=40) # hyperparameter tuning
# rfc.fit(x_train,y_train)
# pred = rfc.predict(x_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, pred)))  # calculate accuracy

print('Micro Precision: {:.2f}'.format(precision_score(y_test, pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, pred, average='weighted')))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, pred, target_names=['Class 1', 'Class 2', 'Class 3']))  # create classification report

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test.argmax(axis =1), pred.argmax(axis =1))  # create confusion matrix
plot_confusion_matrix(conf_mat=mat) # plot confusion matrix

import sklearn.metrics as metrics
print(metrics.confusion_matrix(y_test.flatten(), pred.flatten()))

from google.colab import drive
drive.mount('/content/drive')

import pandas
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

# prepare configuration for cross validation test harness
seed = 7
# prepare models
models = []
models.append(('LR', OneVsRestClassifier(LogisticRegression())))
models.append(('LDA', OneVsRestClassifier(LinearDiscriminantAnalysis())))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
# models.append(('NB', OneVsRestClassifier(GaussianNB())))
models.append(('SVM', OneVsRestClassifier(SVC())))
# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=seed)
	cv_results = model_selection.cross_val_score(model, x, y, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
	print(msg)
# boxplot algorithm comparison
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

#NB,RF,KNN
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.preprocessing import LabelEncoder

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=51)

from sklearn.model_selection import cross_val_score
#model=svm.SVC(kernel='rbf', C=10, gamma='auto')
#model.fit(x,y)
#model.score(x,y)
cross_val_score(svm.SVC(kernel='rbf', C=2, gamma='auto'), X=x_train,y=y_train, cv=2)  # hyperparameter tuning

from sklearn.metrics import confusion_matrix
#confusion = confusion_matrix(metrics.confusion_matrix(y_test.flatten(), pred.flatten()))
#print('Confusion Matrix')
#print(confusion)

#importing accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, pred, average='weighted')))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, pred, target_names=['Class 1', 'Class 2', 'Class 3']))  # create classification report

features = [ 'R1-PA1:VH', 'R1-PM1:V', 'R1-PA2:VH', 'R1-PM2:V', 'R1-PA3:VH', 'R1-PM3:V',
         'R1-PA4:IH', 'R1-PM4:I', 'R1-PA5:IH', 'R1-PM5:I', 'R1-PA6:IH', 'R1-PM6:I', 'R1-PA7:VH',
    'R1-PM7:V', 'R1-PA8:VH', 'R1-PM8:V', 'R1-PA9:VH', 'R1-PM9:V', 'R1-PA10:IH',
  'R1-PM10:I', 'R1-PA11:IH', 'R1-PM11:I', 'R1-PA12:IH', 'R1-PM12:I', 'R1:F', 'R1:DF',
 'R1-PA:Z', 'R1-PA:ZH', 'R1:S', 'R2-PA1:VH', 'R2-PM1:V', 'R2-PA2:VH', 'R2-PM2:V',
'R2-PA3:VH', 'R2-PM3:V', 'R2-PA4:IH', 'R2-PM4:I', 'R2-PA5:IH', 'R2-PM5:I', 'R2-PA6:IH',
'R2-PM6:I', 'R2-PA7:VH', 'R2-PM7:V', 'R2-PA8:VH', 'R2-PM8:V', 'R2-PA9:VH', 'R2-PM9:V',
'R2-PA10:IH', 'R2-PM10:I', 'R2-PA11:IH', 'R2-PM11:I', 'R2-PA12:IH', 'R2-PM12:I',
'R2:F', 'R2:DF', 'R2-PA:Z', 'R2-PA:ZH', 'R2:S', 'R3-PA1:VH', 'R3-PM1:V',
'R3-PA2:VH', 'R3-PM2:V', 'R3-PA3:VH', 'R3-PM3:V', 'R3-PA4:IH', 'R3-PM4:I',
'R3-PA5:IH', 'R3-PM5:I', 'R3-PA6:IH', 'R3-PM6:I', 'R3-PA7:VH', 'R3-PM7:V',
'R3-PA8:VH', 'R3-PM8:V', 'R3-PA9:VH', 'R3-PM9:V', 'R3-PA10:IH', 'R3-PM10:I',
'R3-PA11:IH', 'R3-PM11:I', 'R3-PA12:IH', 'R3-PM12:I', 'R3:F', 'R3:DF', 'R3-PA:Z',
'R3-PA:ZH', 'R3:S' 'R4-PA1:VH', 'R4-PM1:V', 'R4-PA2:VH', 'R4-PM2:V', 'R4-PA3:VH',
'R4-PM3:V', 'R4-PA4:IH', 'R4-PM4:I', 'R4-PA5:IH', 'R4-PM5:I', 'R4-PA6:IH', 'R4-PM6:I',
'R4-PA7:VH', 'R4-PM7:V', 'R4-PA8:VH', 'R4-PM8:V', 'R4-PA9:VH', 'R4-PM9:V', 'R4-PA10:IH',
'R4-PM10:I', 'R4-PA11:IH', 'R4-PM11:I', 'R4-PA12:IH', 'R4-PM12:I', 'R4:F', 'R4:DF',
'R4-PA:Z', 'R4-PA:ZH', 'R4:S', 'control_panel_log1', 'control_panel_log2',
'control_panel_log3', 'control_panel_log4', 'relay1_log', 'relay2_log', 'relay3_log',
'relay4_log', 'snort_log1', 'snort_log2', 'snort_log3', 'snort_log4']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=51)

import tensorflow
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, ELU, Input, Dropout

input = Input(shape=x.shape[1])

m = Dense(64)(input)  # add Dense layer with 64 units
m = ELU()(m)          # add ELU layer with m input
m = Dropout(0.33)(m)  # add dropout layer with 0.33 

m = Dense(64)(m)      # add Dense layer with 64 units and m input
m = ELU()(m)          # add ELU layer with m input
m = Dropout(0.33)(m)  # add dropout layer with 0.33 

m = Dense(32)(m)
m = ELU()(m)
m = Dropout(0.33)(m)

m = Dense(16)(m)
m = ELU()(m)
m = Dropout(0.33)(m)

m = Dense(1, activation='linear')(m)

output = Dense(y.shape[1], activation='softmax')(m)

model = Model(inputs=[input], outputs=[output]) # group all the layers into an object 'model'

model.summary() # print summary of total parameters and trainable parameters

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model.compile(optimizer=Adam(lr=0.03), loss='binary_crossentropy', metrics=['acc'])   # compile model

es = EarlyStopping(monitor='val_loss', patience=100, verbose=1, restore_best_weights=True)  # add early stopping to monitor performance

rlp = ReduceLROnPlateau(monitor='val_loss', patience=9, verbose=1, factor=0.5, cooldown=5, min_lr=1e-10)  # add reduceonpleateau to adjust learning rate

# Commented out IPython magic to ensure Python compatibility.
# # fit model
# %%time
# history = model.fit(x_train
#                     ,y_train
#                     ,validation_data=(x_test,y_test)
#                     ,callbacks=[es, rlp]
#                     ,verbose=1
#                     ,epochs=20
#                     , batch_size=512).history

pred = model.evaluate(x_test, y_test)   # evaluate the model accuracy and loss
pred

loss, accuracy = model.evaluate(x_test, y_test)
print("\nLoss: %.2f, Accuracy: %.2f%%" % (loss, accuracy*100))
pred1 = model.predict(x_test)
pred1=np.argmax(pred1, axis=1)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, pred1)

from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)

accuracy = accuracy_score(y_test, pred1)
recall = recall_score(y_test, pred1)
precision = precision_score(y_test, pred1)
f1 = f1_score(y_test, pred1)

print("confusion matrix")
print("----------------------------------------------")
print("accuracy")
print("%.6f" %accuracy)
print("recall")
print("%.6f" %recall)
print("precision")
print("%.6f" %precision)
print("f1score")
print("%.6f" %f1)

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(y_test.argmax(axis =1), pred.argmax(axis =1))
plot_confusion_matrix(conf_mat=mat)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test, y_test)))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
#print(classification_report(y_test, y_test target_names=['Class 1', 'Class 2', 'Class 3']))

import matplotlib.pyplot as plt
import seaborn as sns

fig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(8, 5))

ax1.plot(history['loss'], label='Train loss')   # plot history of loss under train loss
ax1.plot(history['val_loss'], label='Validation loss')  # plot val_loss under validation loss table 
ax1.legend(loc='best')
ax1.set_title('Loss')

ax2.plot(history['acc'], label='Train accuracy')    # plot accuracy under train accuracy
ax2.plot(history['val_acc'], label='Validation accuracy')   # # plot val_acc under validation accuracy
ax2.legend(loc='best')
ax2.set_title('Accuracy')

plt.xlabel('Epochs')
sns.despine()
plt.show()

